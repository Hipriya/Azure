service_credential = dbutils.secrets.get(scope="<scope>",key="<service-credential-key>")

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", service_credential)
spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", "https://login.microsoftonline.com/<directory-id>/oauth2/token")


# Import necessary modules
from pyspark.sql import SparkSession

# Replace these with your ADLS Gen2 credentials
storage_account_name = "your_storage_account_name"
file_system_name = "your_file_system_name"
directory = "your_directory"
sas_token = "your_sas_token"

# Specify the ADLS URI
adls_uri = f"abfss://{file_system_name}@{storage_account_name}.dfs.core.windows.net/{directory}?{sas_token}"

# Create a Spark session
spark = SparkSession.builder.appName("example").getOrCreate()

# Read data from ADLS into a DataFrame
df = spark.read.csv(adls_uri, header=True, inferSchema=True)

# Show the DataFrame
df.show()

